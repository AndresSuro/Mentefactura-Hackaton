{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de preprocesar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tweet, words):\n",
    "    word_count = {}\n",
    "    tokens = tweet.split()  # Assuming your 'Tokens' field or similar tokenization\n",
    "    for word in words:\n",
    "        word_count[word] = tokens.count(word)\n",
    "    return pd.Series(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer #importamos el tokenizador\n",
    "tweet_token= TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar(text):  #Obtenemos tokens el texto\n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = text.lower()  # Convertimos en minusculas\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])  # quitamos puntación\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = tweet_token.tokenize(text)\n",
    "    #stopwords_list = stopwords.words('spanish')\n",
    "    \n",
    "    #filtered_tokens = [word for word in tokens if word not in stopwords_list]  # Usamos los tokens filatrados\n",
    "    \n",
    "    # Lematizacion\n",
    "    #doc = nlp(\" \".join(filtered_tokens))  #usamos solo los tokens filtrados\n",
    "    #lemmas = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA Debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar participacion función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speech_from_file_improved(file_path, target_speaker, other_speakers):\n",
    "    \n",
    "    collected_text = []\n",
    "    capture = False\n",
    "    found_target_speaker = False\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            normalized_line = line.strip()\n",
    "\n",
    "            if target_speaker in normalized_line:\n",
    "                found_target_speaker = True\n",
    "            \n",
    "            if any(speaker in normalized_line for speaker in other_speakers) and not target_speaker in normalized_line:\n",
    "                if capture:\n",
    "                    capture = False\n",
    "\n",
    "            # Empezar a agreagar hasta que el empiece el candidato objetivo\n",
    "            if found_target_speaker and normalized_line and not any(speaker in normalized_line for speaker in other_speakers):\n",
    "                capture = True\n",
    "                found_target_speaker = False  # Reiniciar para que solo el texto subsecuente sea utilizado\n",
    "\n",
    "            if capture:\n",
    "                collected_text.append(normalized_line)\n",
    "\n",
    "    return \"\\n\".join(collected_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claudia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claudia : Intervenciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_clau = extract_speech_from_file_improved(\"D:\\PythonCode\\LN\\8tavo\\Hackaton\\Debate.txt\",\n",
    "                                   \"Candidata a la Presidencia por la Coalición “Sigamos Haciendo Historia”, Claudia Sheinbaum Pardo\",\n",
    "                                   [\"Candidata a la Presidencia por la Coalición “Fuerza y Corazón por México”, Bertha Xóchitl Gálvez Ruiz\",\n",
    "                                    \"Candidato a la Presidencia del Partido Movimiento Ciudadano, Jorge Álvarez Máynez\",\n",
    "                                    \"Moderador,\",\n",
    "                                    \"Moderadora,\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_clau = result_clau.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_remove = \"Candidata a la Presidencia por la Coalición “Sigamos Haciendo Historia”, Claudia Sheinbaum Pardo:\"\n",
    "\n",
    "cleaned_result_list_clau = [element.replace(phrase_to_remove, \"\") for element in result_list_clau]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_result_list_clau)):\n",
    "    cleaned_result_list_clau[i] = preprocesar(cleaned_result_list_clau[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_list_clau = np.array(cleaned_result_list_clau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('claudia.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for element in cleaned_result_list_clau:\n",
    "        writer.writerow([element])  \n",
    "\n",
    "print(\"CSV file has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Claudia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_lista = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2', max_features=10000, stop_words = stopwords_lista)  #usamos la norma l2 para normalizar y usamos maximo 10000, sobre todos los docuementos\n",
    "lsa_model = TruncatedSVD(n_components=5, random_state=42) # Creamos una instancia de una SVD truncada\n",
    "#con 3 componentes principales que vamos a mantener despues de la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Creamos la matriz TF-IDF para el lugar actual\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_result_list_clau)\n",
    "    \n",
    "# Aplicamos LSA\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "# Extraemos las mejores palabras de cada topico\n",
    "\n",
    "topic_words = []\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_feature_indices = abs(topic).argsort()[-10:][::-1]\n",
    "    top_features = [tfidf_vectorizer.get_feature_names_out()[index] for index in top_feature_indices]\n",
    "    topic_words.append((i+1, top_features))\n",
    "    \n",
    "topics = topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topicos más relevantes, Candidato A :\n",
      "Topico 1: vamos, méxico, ciudad, temas, hicimos, salud, dos, hablar, voy, gobierno\n",
      "Topico 2: vamos, temas, salud, hablar, voy, pública, méxico, plantear, sistema, ciudad\n",
      "Topico 3: méxico, ciudad, sistema, salud, hicimos, pública, supuesto, imss, fortalecer, ley\n",
      "Topico 4: cómo, prian, mentirosa, candidata, creerle, vamos, temas, candidato, hablar, gobierno\n",
      "Topico 5: cómo, prian, candidata, temas, vamos, plantear, hablar, gobierno, dos, tiempo\n"
     ]
    }
   ],
   "source": [
    "print(\"Topicos más relevantes, Candidato A :\")\n",
    "for topic_number, words in topics:\n",
    "        print(f\"Topico {topic_number}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xochitl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xochitl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xoch = extract_speech_from_file_improved(\"D:\\PythonCode\\LN\\8tavo\\Hackaton\\Debate.txt\",\n",
    "                                   \"Candidata a la Presidencia por la Coalición “Fuerza y Corazón por México”, Bertha Xóchitl Gálvez Ruiz\",\n",
    "                                   [\"Candidata a la Presidencia por la Coalición “Sigamos Haciendo Historia”, Claudia Sheinbaum Pardo\",\n",
    "                                    \"Candidato a la Presidencia del Partido Movimiento Ciudadano, Jorge Álvarez Máynez\",\n",
    "                                    \"Moderador,\",\n",
    "                                    \"Moderadora,\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_xoch = result_xoch.split(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_remove = \"Candidata a la Presidencia por la Coalición “Fuerza y Corazón por México”, Bertha Xóchitl Gálvez Ruiz:\"\n",
    "\n",
    "cleaned_result_list_xoch = [element.replace(phrase_to_remove, \"\") for element in result_list_xoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_result_list_xoch)):\n",
    "    cleaned_result_list_xoch[i] = preprocesar(cleaned_result_list_xoch[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_list_xoch = np.array(cleaned_result_list_xoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open('xochitl.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for element in cleaned_result_list_xoch:\n",
    "        writer.writerow([element]) \n",
    "\n",
    "print(\"CSV file has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xochitl LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2', max_features=10000, stop_words = stopwords_lista)  #usamos la norma l2 para normalizar y usamos maximo 10000, sobre todos los docuementos\n",
    "lsa_model = TruncatedSVD(n_components=5, random_state=42) # Creamos una instancia de una SVD truncada\n",
    "#con 3 componentes principales que vamos a mantener despues de la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topicos más relevantes, Candidato B :\n",
      "Topico 1: voy, mujeres, méxico, aquí, claudia, gobierno, corrupción, salud, mujer, va\n",
      "Topico 2: aquí, supuesto, gobierno, mujeres, claudia, inai, colegio, va, mil, pues\n",
      "Topico 3: supuesto, voy, claudia, colegio, hiciste, bueno, castigar, sometería, sociales, todas\n",
      "Topico 4: supuesto, voy, méxico, sometería, salud, educación, mujer, gobierno, presidenta, aquí\n",
      "Topico 5: mujeres, beca, mujer, ningún, voy, sociales, universal, programas, partido, realmente\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Creamos la matriz TF-IDF para el lugar actual\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_result_list_xoch)\n",
    "    \n",
    "# Aplicamos LSA\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "# Extraemos las mejores palabras de cada topico\n",
    "\n",
    "topic_words = []\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_feature_indices = abs(topic).argsort()[-10:][::-1]\n",
    "    top_features = [tfidf_vectorizer.get_feature_names_out()[index] for index in top_feature_indices]\n",
    "    topic_words.append((i+1, top_features))\n",
    "    \n",
    "topics = topic_words\n",
    "print(\"Topicos más relevantes, Candidato B :\")\n",
    "for topic_number, words in topics:\n",
    "        print(f\"Topico {topic_number}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maynez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maynez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_may = extract_speech_from_file_improved(\"D:\\PythonCode\\LN\\8tavo\\Hackaton\\Debate.txt\",\n",
    "                                   \"Candidato a la Presidencia del Partido Movimiento Ciudadano, Jorge Álvarez Máynez\",\n",
    "                                   [\"Candidata a la Presidencia por la Coalición “Sigamos Haciendo Historia”, Claudia Sheinbaum Pardo\",\n",
    "                                    \"Candidata a la Presidencia por la Coalición “Fuerza y Corazón por México”, Bertha Xóchitl Gálvez Ruiz\",\n",
    "                                    \"Moderador,\",\n",
    "                                    \"Moderadora,\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_may = result_may.split(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_remove = \"Candidato a la Presidencia del Partido Movimiento Ciudadano, Jorge Álvarez Máynez:\"\n",
    "\n",
    "cleaned_result_list_may = [element.replace(phrase_to_remove, \"\") for element in result_list_may]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_result_list_may)):\n",
    "    cleaned_result_list_may[i] = preprocesar(cleaned_result_list_may[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_list_may = np.array(cleaned_result_list_may)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open('maynez.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for element in cleaned_result_list_may:\n",
    "        writer.writerow([element]) \n",
    "\n",
    "print(\"CSV file has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maynez LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2', max_features=10000, stop_words = stopwords_lista)  #usamos la norma l2 para normalizar y usamos maximo 10000, sobre todos los docuementos\n",
    "lsa_model = TruncatedSVD(n_components=5, random_state=42) # Creamos una instancia de una SVD truncada\n",
    "#con 3 componentes principales que vamos a mantener despues de la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topicos más relevantes, Candidato C :\n",
      "Topico 1: sistema, méxico, vamos, personas, ser, presidente, política, tema, nuevo, corrupción\n",
      "Topico 2: personas, todas, derechos, modelo, creo, nuevo, sistema, supuesto, corrupción, política\n",
      "Topico 3: niños, millones, niñas, gobierno, pesos, mil, vieja, quiero, política, votan\n",
      "Topico 4: vamos, nuevo, méxico, hijos, todas, derechos, juntos, creo, gobierno, tema\n",
      "Topico 5: bueno, millones, pesos, moral, autoridad, tener, cinco, corrupción, mil, gobierno\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Creamos la matriz TF-IDF para el lugar actual\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_result_list_may)\n",
    "    \n",
    "# Aplicamos LSA\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "# Extraemos las mejores palabras de cada topico\n",
    "\n",
    "topic_words = []\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_feature_indices = abs(topic).argsort()[-10:][::-1]\n",
    "    top_features = [tfidf_vectorizer.get_feature_names_out()[index] for index in top_feature_indices]\n",
    "    topic_words.append((i+1, top_features))\n",
    "    \n",
    "topics = topic_words\n",
    "print(\"Topicos más relevantes, Candidato C :\")\n",
    "\n",
    "for topic_number, words in topics:\n",
    "        print(f\"Topico {topic_number}: {', '.join(words)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
